{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Set Up\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use_precalculated_supporting_data = True\n",
    "# store_calculated_supporting_data = False\n",
    "# use_regressed_z_scores = True\n",
    "# data_directory = '../data/'\n",
    "# all_n_years = ['all', 1, 5]\n",
    "# class_system = 'IPC4'\n",
    "# agent_sample = (0,1000)\n",
    "# # agent_sample = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_n_years_label(n_years, suffix=False):\n",
    "    if n_years is None or n_years=='all' or n_years=='cumulative':\n",
    "        n_years_label = ''\n",
    "        return n_years_label\n",
    "    else:\n",
    "        n_years_label = '%i_years'%n_years\n",
    "    if suffix:\n",
    "        n_years_label = '_'+n_years_label\n",
    "    else:\n",
    "        n_years_label = n_years_label+'_'\n",
    "    return n_years_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Pull in Data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore(data_directory+'organized_patent_data.h5')\n",
    "\n",
    "all_data = store['data_%s'%class_system]\n",
    "classes_lookup = store['classes_lookup_%s'%class_system]\n",
    "agents_lookup = store['agents_lookup_explorers_%s'%class_system]\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = classes_lookup.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take Sample\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 unique agents after sampling\n",
      "34 inventorships after sampling\n"
     ]
    }
   ],
   "source": [
    "if type(agent_sample)==int:\n",
    "    from numpy.random import choice\n",
    "    sampled_agents = agents_lookup.iloc[choice(agents_lookup.shape[0], agent_sample, replace=False)].reset_index()[['Agent_ID', 'Disambiguation_ID']]\n",
    "elif agent_sample is not None:\n",
    "    sampled_agents = agents_lookup.iloc[agent_sample[0]:agent_sample[1]].reset_index()[['Agent_ID', 'Disambiguation_ID']]\n",
    "\n",
    "if agent_sample is not None:\n",
    "    data = all_data.merge(sampled_agents, left_on='Agent', right_on='Disambiguation_ID')\n",
    "\n",
    "print(\"%i unique agents after sampling\"%len(data.Agent_ID.unique()))\n",
    "print(\"%i inventorships after sampling\"%data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_precalculated_supporting_data:\n",
    "    del(all_data)\n",
    "del(data['Disambiguation_ID'])\n",
    "del(data['Agent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_home_classes = data.groupby('Agent_ID')['Class_ID'].first()\n",
    "data['Agent_Home_Class'] = agent_home_classes.ix[data['Agent_ID']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Agents' Performance in Class\n",
    "===\n",
    "(Number of Patents and Citations in Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Agent_Number_of_Patents_in_Class'] = data.groupby(['Agent_ID', 'Class_ID'])['Patent'].transform('count')\n",
    "data['Agent_Number_of_Citations_in_Class'] = data.groupby(['Agent_ID', 'Class_ID'])['Citations'].transform('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Calculate Agent Patent Diversity Data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expanding_diversity(x):\n",
    "    x = x.values\n",
    "    count_dict = {}\n",
    "    diversities = zeros((len(x),2))\n",
    "    for i in range(len(x)-1):\n",
    "        X = x[i]\n",
    "        try:\n",
    "            count_dict[X] +=1\n",
    "        except:\n",
    "            count_dict[X] = 1\n",
    "            \n",
    "        cum_counts = array(list(count_dict.values()))\n",
    "        shares = cum_counts/sum(cum_counts)\n",
    "        \n",
    "        diversities[i+1,0] = sum(shares**2) #First column is Herfindal\n",
    "        diversities[i+1,1] = sum(shares*-log(shares)) #Second column is entropy\n",
    "        #We leave the first row as 0. We are measuring how much diversity there was immediately BEFORE this step.\n",
    "    return pd.DataFrame(diversities)\n",
    "\n",
    "z = data.groupby(['Agent_ID'])['Class_ID'].apply(expanding_diversity)\n",
    "data['Agent_Class_Diversity_Herfindahl_Index'] = z[0].values.astype('float32')\n",
    "data['Agent_Class_Diversity_Entropy'] = z[1].values.astype('float32')\n",
    "del(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify when agents enter into new classes\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_class_entries = data.ix[(data['New_Class']>0)]#*(data['Agent_Class_Number']>1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Expand dataframe to include the classes the agents didn't enter\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Mark which classes were actually entered\n",
    "new_class_data = new_class_entries.copy()\n",
    "new_class_data['Entered'] = True\n",
    "new_class_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "For every new entry, create a list of every class that COULD have been entered\n",
    "I.e. all the unentered classes at that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define the function that identifies and spits out the classes that could have been entered,\n",
    "### creating the new index for the data frame\n",
    "def multiindex_maker(input_line,input_data, \n",
    "                     output_line, output_data,\n",
    "                     function_to_apply=None,\n",
    "                     function_data=None):\n",
    "    \n",
    "    agent = input_data[input_line,0]\n",
    "    entry_number = input_data[input_line,1]\n",
    "    if entry_number == 1: \n",
    "        ### Skip the first class entry, as that's not what we're modeling.\n",
    "        ### Later on, not having these rows in the multiindex will remove these starting points\n",
    "        ### From the data to be analyzed\n",
    "        return output_line\n",
    "\n",
    "    classes_previously_entered = input_data[1+input_line-entry_number:input_line,2] #Agent_Class_Number is 1-indexed\n",
    "    classes_available = sort(list(set(range(n_classes)).difference(classes_previously_entered)))\n",
    "    n_reps = len(classes_available)\n",
    "    \n",
    "    output_data[output_line:output_line+n_reps,0] = agent\n",
    "    output_data[output_line:output_line+n_reps,1] = entry_number\n",
    "    output_data[output_line:output_line+n_reps,2] = classes_available\n",
    "    \n",
    "    if function_to_apply:\n",
    "        function_to_apply(input_line,input_data, \n",
    "                          output_line, output_data,\n",
    "                          classes_previously_entered,\n",
    "                          classes_available,\n",
    "                          n_reps,\n",
    "                          function_data)\n",
    "    return output_line + n_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "n_rows_to_be_created = sum(new_class_entries.Agent_ID.value_counts().map(lambda n: sum(n_classes-1-arange(n-1))))\n",
    "\n",
    "input_data = new_class_entries.sort(['Agent_ID', \n",
    "                                     'Agent_Class_Number'])[['Agent_ID', \n",
    "                                                             'Agent_Class_Number', \n",
    "                                                             'Class_ID']].values\n",
    "output_data = zeros((n_rows_to_be_created, 3))\n",
    "\n",
    "output_line = 0\n",
    "for input_line in arange(len(input_data)):\n",
    "    output_line = multiindex_maker(input_line, input_data, output_line, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### We now have a set of every agent, their class entry numbers, and the classes they\n",
    "### could have entered at that time. We make this into a multiindex, and reindex the\n",
    "### new class data to use that index. All the blank spots (i.e. the unentered classes)\n",
    "### will be added to the data frame as empty rows (nans)\n",
    "multiindex = output_data\n",
    "\n",
    "multiindex = pd.MultiIndex.from_arrays([multiindex[:,0],\n",
    "                                        multiindex[:,1],\n",
    "                                        multiindex[:,2]], \n",
    "              names=['Agent_ID','Agent_Class_Number','Class_ID'])\n",
    "\n",
    "new_class_data.set_index(['Agent_ID', 'Agent_Class_Number', 'Class_ID'], inplace=True)\n",
    "new_class_data = new_class_data.reindex(multiindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in new_class_data.columns:\n",
    "    if new_class_data[c].dtype == 'float64':\n",
    "        new_class_data[c] = new_class_data[c].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_class_data.reset_index(inplace=True)\n",
    "new_class_data.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Fill in missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in ['Application_Year', 'Issued_Year', 'Application_Date', \n",
    "            'Agent_Class_Diversity_Herfindahl_Index', 'Agent_Class_Diversity_Entropy',\n",
    "            'Agent_Number_of_Patents_in_Class', 'Agent_Number_of_Citations_in_Class',\n",
    "            'Agent_Number_of_Classes_All_Time', 'Agent_Number_of_Patents_All_Time',\n",
    "            'Agent_Patent_Number'\n",
    "           ]:\n",
    "    new_class_data[col] = new_class_data.groupby(['Agent_ID', \n",
    "                                                 'Agent_Class_Number'])[col].transform(\n",
    "                                                    lambda x: x.fillna(\n",
    "                                                        x[x.first_valid_index()]\n",
    "                                                    )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_class_data['Entered'].fillna(False, inplace=True)\n",
    "new_class_data['Entered'] = new_class_data['Entered'].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for c in new_class_data.columns:\n",
    "    if new_class_data[c].dtype == 'float64':\n",
    "        new_class_data[c] = new_class_data[c].astype('float32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Make simple predictors\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Code to rank predictors, once we calculate them.\n",
    "def rankify(column_name,data=new_class_data):\n",
    "    data[column_name+'_Percentile'] = (100. * (data.groupby(['Agent_ID','Agent_Class_Number'])[column_name].rank() / \n",
    "                                              data.groupby(['Agent_ID','Agent_Class_Number'])[column_name].transform('count')\n",
    "                                              )).astype('float32')\n",
    "    data.drop(column_name, axis=1, inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Class' popularity up until the inventor entered them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class_counts_dict = {}\n",
    "# class_by_year_multiindex = pd.MultiIndex.from_product((classes_lookup['Class_ID'], \n",
    "#                                                        range(all_data.Issued_Year.min(), \n",
    "#                                                              all_data.Issued_Year.max()+1)),\n",
    "#                                                       names=('Class_ID', 'Issued_Year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ### Patent count\n",
    "# class_counts_dict['Class_Patent_Count'] = all_data.groupby(['Class_ID', 'Issued_Year'])['Patent'].nunique()\n",
    "# class_counts_dict['Class_Patent_Count'] = class_counts_dict['Class_Patent_Count'].reindex(class_by_year_multiindex).fillna(0)\n",
    "\n",
    "# class_counts_dict['Class_Patent_Count_Cumulative'] = class_counts_dict['Class_Patent_Count'].groupby(level='Class_ID').cumsum()\n",
    "# class_counts_dict['Class_Patent_Count_Cumulative'].index.name = 'Class_ID'\n",
    "\n",
    "\n",
    "# # class_counts_dict['Class_Patent_Count'] class_size.groupby(level='Class').apply(lambda x: pd.rolling_sum(x, n_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ### Agent count per year (repeats agents from year to year)\n",
    "# class_counts_dict['Class_Agent_Count'] = all_data.groupby(['Class_ID', 'Issued_Year'])['Agent'].nunique()\n",
    "# class_counts_dict['Class_Agent_Count'] = class_counts_dict['Class_Agent_Count'].reindex(class_by_year_multiindex).fillna(0)\n",
    "\n",
    "# ### Agent new count by year (no repeats of agents from year to year)\n",
    "# class_counts_dict['Class_New_Agent_Count'] = new_class_entries.groupby(['Class_ID', 'Issued_Year'])['Agent_ID'].count()\n",
    "# class_counts_dict['Class_New_Agent_Count'] = class_counts_dict['Class_New_Agent_Count'].reindex(class_by_year_multiindex).fillna(0)\n",
    "\n",
    "\n",
    "# ### Agent cumulative count by year (no repeats of agents from year to year)\n",
    "# class_counts_dict['Class_New_Agent_Count_Cumulative'] = class_counts_dict['Class_New_Agent_Count'].groupby(level='Class_ID').cumsum()\n",
    "# class_counts_dict['Class_New_Agent_Count_Cumulative'].index.name = 'Class_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "class_counts_dict = {}\n",
    "\n",
    "counts_data = pd.HDFStore(data_directory+'popularity_counts.h5')\n",
    "for n_years in all_n_years:\n",
    "    print(n_years)\n",
    "    n_years_label = create_n_years_label(n_years)\n",
    "    n_years_label_suffix = create_n_years_label(n_years,suffix=True)\n",
    "    class_counts_dict['Class_Patent_Count%s'%n_years_label_suffix] = counts_data['patent_count_%s%s'%(n_years_label, \n",
    "                                                                                   class_system)]\n",
    "    class_counts_dict['Class_New_Inventor_Count%s'%n_years_label_suffix] = counts_data['new_inventor_count_%s%s'%(n_years_label, \n",
    "                                                                                   class_system)]\n",
    "\n",
    "counts_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in class_counts_dict.keys():\n",
    "    class_counts_dict[k] = class_counts_dict[k].reset_index()\n",
    "    class_counts_dict[k].rename(columns={'Agent_ID': 'Count',\n",
    "                                         'Agent': 'Count',\n",
    "                                         'Patent': 'Count',\n",
    "                                         'patent': 'Count',\n",
    "                                         'inventor': 'Count',\n",
    "                                         'Year': 'Issued_Year',\n",
    "                                         0: 'Count'\n",
    "                                         }, inplace=True)\n",
    "\n",
    "    #Imported data will use class names, which should be converted to numeric class ids using our lookup table\n",
    "    if 'Class' in class_counts_dict[k].columns and 'Class_ID' not in class_counts_dict[k].columns:\n",
    "        class_counts_dict[k]['Class_ID'] = classes_lookup.set_index('Class_Name').ix[class_counts_dict[k]['Class']].values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in class_counts_dict.keys():\n",
    "    class_counts_dict[k]['Application_Year'] = class_counts_dict[k]['Issued_Year']+1 \n",
    "    #We're calling this \"Application_Year\", but really it's the application year it will be paired with.\n",
    "    #It's the application year that will have this information present. (Note the **+1**)\n",
    "    new_class_data[k+'_Previous_Year'] = new_class_data[['Class_ID','Application_Year']].merge(class_counts_dict[k],\n",
    "                                      on=['Class_ID', 'Application_Year'],\n",
    "                                      how='left')['Count'].fillna(0)\n",
    "    rankify(k+'_Previous_Year',data=new_class_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Make network predictors\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_connectivities(connectivity_metric,\n",
    "#                              connectivity_metric_prefix='',\n",
    "                             connectivity_metric_suffix='',\n",
    "                             networks=None,\n",
    "                             new_class_data=new_class_data,\n",
    "                             new_class_entries=new_class_entries,\n",
    "                             use_the_future=False,\n",
    "                             combination_method='mean',\n",
    "                             additional_label=''\n",
    "                             ):\n",
    "\n",
    "    n_rows_to_be_created = new_class_data.shape[0]\n",
    "\n",
    "    input_data = new_class_entries.sort(['Agent_ID', \n",
    "                                         'Agent_Class_Number'])[['Agent_ID', \n",
    "                                                                 'Agent_Class_Number', \n",
    "                                                                 'Class_ID', \n",
    "                                                                 'Application_Year']].values\n",
    "    if use_the_future:\n",
    "       input_data[:,3] = 0\n",
    "    else:\n",
    "        year_lookup = pd.DataFrame(index=networks.items,\n",
    "                                   columns=['Year_ID'],\n",
    "                                   data=arange(len(networks.items))\n",
    "                                   )\n",
    "        input_data[:,3] = year_lookup.ix[input_data[:,3]-1, 'Year_ID']\n",
    "    output_data = zeros((n_rows_to_be_created, 4))\n",
    "    \n",
    "    network_arrays = networks.ix[connectivity_metric].values\n",
    "\n",
    "    output_line = 0\n",
    "    for input_line in arange(len(input_data)):\n",
    "        output_line = multiindex_maker(input_line, input_data, \n",
    "                                       output_line, output_data,\n",
    "                                       function_to_apply= lambda *args: calculate_connectivities_helper(*args, \n",
    "                                                                                                       combination_method=combination_method),\n",
    "                                       function_data=network_arrays)\n",
    "\n",
    "    new_class_data.sort(['Agent_ID', 'Agent_Class_Number'], inplace=True)\n",
    "    new_class_data[connectivity_metric+connectivity_metric_suffix+'_'+additional_label+'_'+combination_method] = output_data[:,3]\n",
    "    return\n",
    "\n",
    "def calculate_connectivities_helper(input_line, input_data, \n",
    "                                    output_line, output_data,\n",
    "                                    classes_previously_entered,\n",
    "                                    classes_available,\n",
    "                                    n_reps,\n",
    "                                    network_arrays,\n",
    "                                    combination_method):\n",
    "\n",
    "    year_index = input_data[input_line,3]\n",
    "    if isnan(year_index) or year_index==-9223372036854775808: #If we asked for a year that we don't have network information on\n",
    "        connectivity_from_previous = 0 #Sorry, can't help you.\n",
    "    else:\n",
    "        connectivity_from_previous = network_arrays[year_index][classes_previously_entered][:,classes_available]\n",
    "        if combination_method=='mean':\n",
    "            connectivity_from_previous = connectivity_from_previous.mean(axis=0) \n",
    "        if combination_method=='max':\n",
    "            connectivity_from_previous = connectivity_from_previous.max(axis=0)  \n",
    "\n",
    "    output_data[output_line:output_line+n_reps,3] = connectivity_from_previous \n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/class_relatedness_networks.h5',mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "Class_Cites_Class_Count"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:39: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_PID\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "1\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "5\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n"
     ]
    }
   ],
   "source": [
    "summary_statistic_label='z_score'\n",
    "\n",
    "if use_regressed_z_scores:\n",
    "    regression_label = 'regressed_'\n",
    "else:\n",
    "    regression_label = ''\n",
    "\n",
    "for n_years in all_n_years:\n",
    "    print(n_years)\n",
    "    n_years_label = create_n_years_label(n_years)\n",
    "    network_to_use = 'empirical_z_scores_%s%s%s'%(regression_label, n_years_label, class_system)\n",
    "    networks = store[network_to_use]\n",
    "    networks.fillna(0,inplace=True) #Interpreting nan z-scores as 0\n",
    "    ### Make sure the networks' arrays are ordered in the same sequence that our classes are ordered\n",
    "    class_id_sequence = classes_lookup.sort(['Class_ID'])['Class_Name']\n",
    "    networks = networks.reindex(major_axis=class_id_sequence, \n",
    "                                minor_axis=class_id_sequence)\n",
    "    \n",
    "    for connectivity_metric in networks.labels:\n",
    "        print(connectivity_metric)\n",
    "        for combination_method in ['mean']:#, 'max']:\n",
    "            connectivity_metric_suffix = create_n_years_label(n_years,suffix=True)\n",
    "            calculate_connectivities(connectivity_metric,\n",
    "                                     connectivity_metric_suffix,\n",
    "                                     networks=networks,\n",
    "                                     use_the_future=False,\n",
    "                                     combination_method=combination_method,\n",
    "                                     additional_label=summary_statistic_label\n",
    "                                    )\n",
    "#             rankify(connectivity_metric+connectivity_metric_suffix+'_'+summary_statistic_label+'_'+combination_method, new_class_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summary_statistic_label = 'percent_positive'\n",
    "# regression_label = ''\n",
    "\n",
    "# for n_years in all_n_years:\n",
    "#     print(n_years)\n",
    "#     n_years_label = create_n_years_label(n_years)\n",
    "#     network_to_use = 'empirical_z_scores_%s%s%s'%(regression_label, n_years_label, class_system)\n",
    "#     networks = store[network_to_use]\n",
    "#     networks.fillna(0,inplace=True) #Interpreting nan z-scores as 0\n",
    "#     ### Make sure the networks' arrays are ordered in the same sequence that our classes are ordered\n",
    "#     class_id_sequence = classes_lookup.sort(['Class_ID'])['Class_Name']\n",
    "#     networks = networks.reindex(major_axis=class_id_sequence, \n",
    "#                                 minor_axis=class_id_sequence)\n",
    "#     networks = networks>0\n",
    "#     networks = networks.cumsum(axis=1)\n",
    "    \n",
    "#     n = networks.copy()\n",
    "#     for i in arange(len(n.items)):\n",
    "#         n.iloc[:,i] = i+1\n",
    "#     networks = networks.divide(n)\n",
    "    \n",
    "#     for connectivity_metric in networks.labels:\n",
    "#         print(connectivity_metric)\n",
    "#         for combination_method in ['mean']:#, 'max']:\n",
    "#             connectivity_metric_suffix = create_n_years_label(n_years,suffix=True)\n",
    "#             calculate_connectivities(connectivity_metric,\n",
    "#                                      connectivity_metric_suffix,\n",
    "#                                      networks=networks,\n",
    "#                                      use_the_future=False,\n",
    "#                                      combination_method=combination_method,\n",
    "#                                      additional_label=summary_statistic_label\n",
    "#                                     )\n",
    "# #             rankify(connectivity_metric+connectivity_metric_suffix+'percent_positive'+combination_method, new_class_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "Class_Cites_Class_Count"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:10: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:39: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "summary_statistic_label = 'percent_positive'\n",
    "regression_label = ''\n",
    "\n",
    "n_years = 1\n",
    "n_years_label = create_n_years_label(n_years)\n",
    "network_to_use = 'empirical_z_scores_%s%s%s'%(regression_label, n_years_label, class_system)\n",
    "networks = store[network_to_use]\n",
    "networks.fillna(0,inplace=True) #Interpreting nan z-scores as 0\n",
    "### Make sure the networks' arrays are ordered in the same sequence that our classes are ordered\n",
    "class_id_sequence = classes_lookup.sort(['Class_ID'])['Class_Name']\n",
    "networks = networks.reindex(major_axis=class_id_sequence, \n",
    "                            minor_axis=class_id_sequence)\n",
    "networks = networks>0\n",
    "\n",
    "\n",
    "for n_years_back in ['all']: #all_n_years:\n",
    "    print(n_years_back)\n",
    "    if n_years_back==1:\n",
    "        these_networks = networks\n",
    "    elif n_years_back=='all':\n",
    "#         continue #We already calculated this in the cell above.\n",
    "        these_networks = networks.cumsum(axis=1)\n",
    "        r = these_networks.copy()        \n",
    "        n = these_networks.copy()\n",
    "        for i in arange(len(n.items)):\n",
    "            n.iloc[:,i] = i+1\n",
    "        these_networks = r.divide(n)\n",
    "\n",
    "    else:\n",
    "        these_networks = networks.cumsum(axis=1)\n",
    "        \n",
    "        r = these_networks.copy()\n",
    "        #Lower the sums to only consider n_years_back\n",
    "        s = (r.iloc[:,n_years_back:].values-r.iloc[:,:-n_years_back].values) \n",
    "        r.iloc[:,n_years_back:] = s\n",
    "        \n",
    "        n = these_networks.copy()\n",
    "        for i in arange(len(n.items)):\n",
    "            n.iloc[:,i] = min(i+1, n_years_back)\n",
    "        these_networks = r.divide(n)\n",
    "\n",
    "    \n",
    "    for connectivity_metric in networks.labels:\n",
    "        print(connectivity_metric)\n",
    "        for combination_method in ['mean']:#, 'max']:\n",
    "            connectivity_metric_suffix = create_n_years_label(n_years,suffix=True)\n",
    "            calculate_connectivities(connectivity_metric,\n",
    "                                     connectivity_metric_suffix,\n",
    "                                     networks=these_networks,\n",
    "                                     use_the_future=False,\n",
    "                                     combination_method=combination_method,\n",
    "                                     additional_label=summary_statistic_label+'_%s_years_back'%str(n_years_back)\n",
    "                                    )\n",
    "    #       rankify(connectivity_metric+connectivity_metric_suffix+'percent_positive'+combination_method, new_class_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "Class_Cites_Class_Count"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:10: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:39: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "summary_statistic_label = 'percent_positive'\n",
    "regression_label = ''\n",
    "\n",
    "n_years = 1\n",
    "n_years_label = create_n_years_label(n_years)\n",
    "network_to_use = 'empirical_z_scores_%s%s%s'%(regression_label, n_years_label, class_system)\n",
    "networks = store[network_to_use]\n",
    "networks.fillna(0,inplace=True) #Interpreting nan z-scores as 0\n",
    "### Make sure the networks' arrays are ordered in the same sequence that our classes are ordered\n",
    "class_id_sequence = classes_lookup.sort(['Class_ID'])['Class_Name']\n",
    "networks = networks.reindex(major_axis=class_id_sequence, \n",
    "                            minor_axis=class_id_sequence)\n",
    "networks_unthresholded = networks.copy()\n",
    "\n",
    "for threshold in [1,2,3,5,6,9]:\n",
    "    networks = networks_unthresholded>threshold\n",
    "\n",
    "    for n_years_back in ['all']:#all_n_years:\n",
    "        print(n_years_back)\n",
    "        if n_years_back==1:\n",
    "            these_networks = networks\n",
    "        elif n_years_back=='all':\n",
    "    #         continue #We already calculated this in the cell above.\n",
    "            these_networks = networks.cumsum(axis=1)\n",
    "            r = these_networks.copy()        \n",
    "            n = these_networks.copy()\n",
    "            for i in arange(len(n.items)):\n",
    "                n.iloc[:,i] = i+1\n",
    "            these_networks = r.divide(n)\n",
    "\n",
    "        else:\n",
    "            these_networks = networks.cumsum(axis=1)\n",
    "\n",
    "            r = these_networks.copy()\n",
    "            #Lower the sums to only consider n_years_back\n",
    "            s = (r.iloc[:,n_years_back:].values-r.iloc[:,:-n_years_back].values) \n",
    "            r.iloc[:,n_years_back:] = s\n",
    "\n",
    "            n = these_networks.copy()\n",
    "            for i in arange(len(n.items)):\n",
    "                n.iloc[:,i] = min(i+1, n_years_back)\n",
    "            these_networks = r.divide(n)\n",
    "\n",
    "\n",
    "        for connectivity_metric in networks.labels:\n",
    "            print(connectivity_metric)\n",
    "            for combination_method in ['mean']:#, 'max']:\n",
    "                connectivity_metric_suffix = create_n_years_label(n_years,suffix=True)\n",
    "                calculate_connectivities(connectivity_metric,\n",
    "                                         connectivity_metric_suffix,\n",
    "                                         networks=these_networks,\n",
    "                                         use_the_future=False,\n",
    "                                         combination_method=combination_method,\n",
    "                                         additional_label=summary_statistic_label+'_%s_years_back_thr_%iz'%(str(n_years_back),\n",
    "                                                                                                            threshold)\n",
    "                                                                                                           \n",
    "                                        )\n",
    "        #       rankify(connectivity_metric+connectivity_metric_suffix+'percent_positive'+combination_method, new_class_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "Class_Cites_Class_Count"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:10: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n",
      "all\n",
      "Class_Cites_Class_Count\n",
      "Class_Cited_by_Class_Count\n",
      "Class_CoOccurrence_Count_Firm\n",
      "Class_CoOccurrence_Count_Inventor\n",
      "Class_CoOccurrence_Count_PID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:39: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "summary_statistic_label = 'percent_positive'\n",
    "regression_label = 'regressed_'\n",
    "\n",
    "n_years = 1\n",
    "n_years_label = create_n_years_label(n_years)\n",
    "network_to_use = 'empirical_z_scores_%s%s%s'%(regression_label, n_years_label, class_system)\n",
    "networks = store[network_to_use]\n",
    "networks.fillna(0,inplace=True) #Interpreting nan z-scores as 0\n",
    "### Make sure the networks' arrays are ordered in the same sequence that our classes are ordered\n",
    "class_id_sequence = classes_lookup.sort(['Class_ID'])['Class_Name']\n",
    "networks = networks.reindex(major_axis=class_id_sequence, \n",
    "                            minor_axis=class_id_sequence)\n",
    "networks_unthresholded = networks.copy()\n",
    "\n",
    "for threshold in [1,2,3,5,6,9]:\n",
    "    networks = networks_unthresholded>threshold\n",
    "\n",
    "    for n_years_back in ['all']:#all_n_years:\n",
    "        print(n_years_back)\n",
    "        if n_years_back==1:\n",
    "            these_networks = networks\n",
    "        elif n_years_back=='all':\n",
    "    #         continue #We already calculated this in the cell above.\n",
    "            these_networks = networks.cumsum(axis=1)\n",
    "            r = these_networks.copy()        \n",
    "            n = these_networks.copy()\n",
    "            for i in arange(len(n.items)):\n",
    "                n.iloc[:,i] = i+1\n",
    "            these_networks = r.divide(n)\n",
    "\n",
    "        else:\n",
    "            these_networks = networks.cumsum(axis=1)\n",
    "\n",
    "            r = these_networks.copy()\n",
    "            #Lower the sums to only consider n_years_back\n",
    "            s = (r.iloc[:,n_years_back:].values-r.iloc[:,:-n_years_back].values) \n",
    "            r.iloc[:,n_years_back:] = s\n",
    "\n",
    "            n = these_networks.copy()\n",
    "            for i in arange(len(n.items)):\n",
    "                n.iloc[:,i] = min(i+1, n_years_back)\n",
    "            these_networks = r.divide(n)\n",
    "\n",
    "\n",
    "        for connectivity_metric in networks.labels:\n",
    "            print(connectivity_metric)\n",
    "            for combination_method in ['mean']:#, 'max']:\n",
    "                connectivity_metric_suffix = create_n_years_label(n_years,suffix=True)\n",
    "                calculate_connectivities(connectivity_metric,\n",
    "                                         connectivity_metric_suffix,\n",
    "                                         networks=these_networks,\n",
    "                                         use_the_future=False,\n",
    "                                         combination_method=combination_method,\n",
    "                                         additional_label=summary_statistic_label+'_%s_years_back_thr_%iz_regressed'%(str(n_years_back),\n",
    "                                                                                                            threshold)\n",
    "                                                                                                           \n",
    "                                        )\n",
    "        #       rankify(connectivity_metric+connectivity_metric_suffix+'percent_positive'+combination_method, new_class_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Make citation count predictors\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Class citation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_precalculated_supporting_data:\n",
    "    class_citation_count = pd.read_hdf(data_directory+'class_citation_counts.h5', \n",
    "                                              class_system)\n",
    "\n",
    "    class_citation_count_lookup = pd.read_hdf(data_directory+'class_citation_counts.h5', \n",
    "                                              'class_lookup_table_%s'%class_system)\n",
    "\n",
    "    class_citation_count['Class_ID'] = classes_lookup.set_index('Class_Name').ix[ \\\n",
    "        class_citation_count_lookup.ix[class_citation_count['Class_ID'], 'index'].values \\\n",
    "                                                                                ]['Class_ID'].values\n",
    "\n",
    "\n",
    "    measures = [m for m in class_citation_count if m not in ['Class_ID', 'Issued_Year']]\n",
    "    class_citation_count.rename(columns=dict([(m, m+'_Previous_Year') for m in measures]), inplace=True)\n",
    "    \n",
    "    class_citation_count.rename(columns={'Issued_Year': 'Application_Year'}, inplace=True)\n",
    "    class_citation_count['Application_Year'] = class_citation_count['Application_Year']+1\n",
    "    #We're calling this \"Application_Year\", but really it's the application year it will be paired with.\n",
    "    #It's the application year that will have this information present. \n",
    "    \n",
    "    new_class_data = new_class_data.merge(class_citation_count, on=['Class_ID', 'Application_Year'], how='left').fillna(0)\n",
    "    for m in measures: \n",
    "        rankify(m+'_Previous_Year',data=new_class_data)\n",
    "        \n",
    "else:\n",
    "    store = pd.HDFStore(data_directory+'citations_organized.h5')\n",
    "    citations = store['citations']\n",
    "    citation_class_lookup = store['%s_class_lookup'%class_system]\n",
    "    citation_class_lookup = citation_class_lookup.reset_index().set_index(0)\n",
    "    store.close()\n",
    "\n",
    "    for column in citations.columns:\n",
    "        if class_system in column:\n",
    "            new_name = column.replace('_'+class_system, \"\")\n",
    "            citations.rename(columns={column: new_name}, inplace=True)\n",
    "\n",
    "    class_citations_dict = {}\n",
    "\n",
    "    for citation_type, class_column in [('Outward', 'Class_Citing_Patent'),\n",
    "                                        ('Inward', 'Class_Cited_Patent')]:\n",
    "        count_by_year = citations.groupby([class_column, 'Year_Citing_Patent'])['Citing_Patent'].count().sort_index([class_column, 'Year_Citing_Patent'])\n",
    "        cumulative_count_by_year = count_by_year.groupby(level=class_column).cumsum().reset_index()\n",
    "        count_by_year = count_by_year.reset_index()\n",
    "\n",
    "        class_citations_dict['Class_%s_Citation_Count'%citation_type] = count_by_year.reset_index().rename(columns={class_column:'Class_ID',\n",
    "                                                                                                                                    'Citing_Patent': 'Count'})\n",
    "        class_citations_dict['Class_Cumulative_%s_Citation_Count'%citation_type] = cumulative_count_by_year.rename(columns={0:'Count',\n",
    "                                                                                                                                             class_column:'Class_ID'})\n",
    "    for k in class_citations_dict.keys():\n",
    "        class_citations_dict[k].rename(columns={'Year_Citing_Patent': 'Issued_Year'}, inplace=True)\n",
    "\n",
    "        #The stored citations data may have a different class_lookup index than we have calculated here\n",
    "        #so we convert it to ours.\n",
    "        class_citations_dict[k]['Class_ID'] = classes_lookup.set_index('Class_Name').ix[citation_class_lookup.ix[\n",
    "                class_citations_dict[k]['Class_ID']]['index']]['Class_ID'].values\n",
    "    for k in class_citations_dict.keys():\n",
    "\n",
    "        class_citations_dict[k]['Application_Year'] = class_citations_dict[k]['Issued_Year']+1 \n",
    "        #We're calling this \"Application_Year\", but really it's the application year it will be paired with.\n",
    "        #It's the application year that will have this information present. \n",
    "        new_class_data[k+'_Previous_Year'] = new_class_data[['Class_ID','Application_Year']].merge(class_citations_dict[k],\n",
    "                                          on=['Class_ID', 'Application_Year'],\n",
    "                                          how='left')['Count'].fillna(0)\n",
    "        rankify(k+'_Previous_Year', new_class_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Agent citation counts to individual classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Agent citation counts to individual classes\n",
    "def calculate_agent_citation_counts(citation_data,\n",
    "                                    inventorships,\n",
    "                                    agents_lookup=agents_lookup,\n",
    "                                    new_class_data=new_class_data,\n",
    "                                    new_class_entries=new_class_entries,\n",
    "                                    ):\n",
    "\n",
    "    n_rows_to_be_created = new_class_data.shape[0]\n",
    "    input_data = new_class_entries.sort(['Agent_ID', \n",
    "                                         'Agent_Class_Number'])[['Agent_ID', \n",
    "                                                                 'Agent_Class_Number', \n",
    "                                                                 'Class_ID', \n",
    "                                                                 'Application_Year']].values \n",
    "    output_data = zeros((n_rows_to_be_created, 4))\n",
    "    output_line = 0\n",
    "    for input_line in arange(len(input_data)):\n",
    "        output_line = multiindex_maker(input_line, input_data, \n",
    "                                       output_line, output_data,\n",
    "                                       function_to_apply=calculate_agent_citation_counts_helper,\n",
    "                                       function_data=(citation_data,\n",
    "                                                      agents_lookup,\n",
    "                                                      inventorships)\n",
    "                        )\n",
    "\n",
    "    new_class_data.sort(['Agent_ID', 'Agent_Class_Number'], inplace=True)\n",
    "    new_class_data['Agent_Previous_Citations_to_Class'] = output_data[:,3]\n",
    "#     new_class_data['Agent_Previous_Citations_from_Class'] = output_data[:,4]\n",
    "    return\n",
    "\n",
    "def calculate_agent_citation_counts_helper(input_line, input_data, \n",
    "                                    output_line, output_data,\n",
    "                                    classes_previously_entered,\n",
    "                                    classes_available,\n",
    "                                    n_reps,\n",
    "                                    other_data):\n",
    "    \n",
    "    year = input_data[input_line,3]\n",
    "    agent = input_data[input_line,0]\n",
    "    \n",
    "    patent_class_citation_count, agents_lookup, inventorships = other_data    \n",
    "    \n",
    "#     agent = agents_lookup.ix[agent,'Disambiguation_ID']\n",
    "    agents_patents = inventorships.ix[agent, ['Patent', 'Issued_Year']]\n",
    "    earlier_agents_patents = agents_patents[agents_patents['Issued_Year']<year]['Patent'].values\n",
    "\n",
    "    #citations['Citing_Patent'].isin(earlier_agents_patents)\n",
    "#     earlier_citations_received = citations['Cited_Patent'].isin(earlier_agents_patents)\n",
    "#     earlier_citations_received = (earlier_citations_received) & (citations['Year_Citing_Patent']<year)\n",
    "    \n",
    "#     print(earlier_citations_made)\n",
    "#     print(earlier_citations_received)\n",
    "#     agent_citations_to_class = citations[earlier_citations_made]['Class_Cited_Patent'].value_counts()\n",
    "#     agent_citations_from_class = citations[earlier_citations_received]['Class_Citing_Patent'].value_counts()\n",
    "\n",
    "#     temp = zeros(n_classes)\n",
    "#     temp[agent_citations_to_class.index.values] = agent_citations_to_class.values\n",
    "#     agent_citations_to_class = temp[classes_available]\n",
    "    \n",
    "#     temp = zeros(n_classes)\n",
    "#     temp[agent_citations_from_class.index.values] = agent_citations_from_class.values\n",
    "#     agent_citations_from_class = temp[classes_available]\n",
    "\n",
    "    earlier_citations_made = patent_class_citation_count.ix[earlier_agents_patents].fillna(0).sum()\n",
    "\n",
    "    output_data[output_line:output_line+n_reps,3] = earlier_citations_made[classes_available].values\n",
    "#     output_data[output_line:output_line+n_reps,3] = agent_citations_from_class\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_precalculated_supporting_data:\n",
    "    patent_class_citation_count = pd.read_hdf(data_directory+'patent_class_citation_count.h5', \n",
    "                                              class_system)\n",
    "    patent_class_citation_count_class_lookup = pd.read_hdf(data_directory+'patent_class_citation_count.h5',\n",
    "                                                           'class_lookup_table_%s'%class_system)\n",
    "    patent_class_citation_count_class_lookup = pd.DataFrame(patent_class_citation_count_class_lookup).reset_index()\n",
    "else:\n",
    "    patent_class_citation_count = citations.groupby('Citing_Patent')['Class_Cited_Patent'].value_counts()\n",
    "    patent_class_citation_count_class_lookup = citation_class_lookup\n",
    "\n",
    "if store_calculated_supporting_data:\n",
    "    patent_class_citation_count.to_hdf(data_directory+'patent_class_citation_count.h5', class_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-ed2131c59606>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpatent_class_citation_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatent_class_citation_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36munstack\u001b[1;34m(self, level)\u001b[0m\n\u001b[0;32m   1993\u001b[0m         \"\"\"\n\u001b[0;32m   1994\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m     \u001b[1;31m#----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/pandas/core/reshape.py\u001b[0m in \u001b[0;36munstack\u001b[1;34m(obj, level)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[0munstacker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Unstacker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0munstacker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/pandas/core/reshape.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;31m# TODO: find a better way than this masking business\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_new_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_new_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_new_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jeffrey_alstott/anaconda3/lib/python3.4/site-packages/pandas/core/reshape.py\u001b[0m in \u001b[0;36mget_new_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_promote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[0mnew_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patent_class_citation_count = patent_class_citation_count.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Convert patent_class_citation_count's class IDs to the same ones we're using here.\n",
    "patent_class_citation_count.columns = classes_lookup.set_index('Class_Name').ix[patent_class_citation_count_class_lookup.ix[patent_class_citation_count.columns, \n",
    "                                                                                                         'index'].values]['Class_ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.set_index('Agent_ID', inplace=True)\n",
    "data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_agent_citation_counts(patent_class_citation_count, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rankify('Agent_Previous_Citations_to_Class')\n",
    "# rankify('Agent_Previous_Citations_from_Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Make social network predictors\n",
    "====\n",
    "\"Who of my previous co-authors had patented in that classes, before I patented with them?\"\n",
    "The most computationally intensive part is identifying all of an author's previous co-authors, and storing them. Once that's done you \"just\" need to iterate through each row of new_class_entries, identify the author and year for that entry, then lookup the author's co-authors wherever you stored them, then find THEIR previous entries in new_class_entries (entries that were BEFORE you patented with them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if use_precalculated_supporting_data:\n",
    "    store = pd.HDFStore(data_directory+'agent_patent_relationships.h5',mode='r')\n",
    "    agent_patent_lists = store['agent_patent_lists']\n",
    "    agent_patent_year_lists = store['agent_patent_year_lists']\n",
    "    patent_agent_lists = store['patent_agent_lists']\n",
    "    patent_classes = store['patent_classes_%s'%class_system]\n",
    "    patent_classes_lookup = store['class_lookup_table_%s'%class_system]\n",
    "    store.close()\n",
    "    \n",
    "    patent_classes['Class_ID'] = classes_lookup.set_index('Class_Name').ix[ \\\n",
    "        patent_classes_lookup.ix[patent_classes['Class_ID'], 'Class_Name'].values \\\n",
    "                                                                          ]['Class_ID'].values\n",
    "    \n",
    "else:\n",
    "    agent_patent_lists = all_data.groupby(level='Agent')['Patent'].apply(lambda x: list(x))\n",
    "    agent_patent_year_lists = all_data.groupby(level='Agent')['Issued_Year'].apply(lambda x: list(x))\n",
    "    patent_agent_lists = all_data.reset_index().groupby('Patent')['Agent'].apply(lambda x: list(x))\n",
    "    patent_classes = all_data.drop_duplicates('Patent')[['Patent', 'Application_Year', 'Class_ID']].set_index('Patent')\n",
    "\n",
    "if store_calculated_supporting_data:\n",
    "    store = pd.HDFStore(data_directory+'agent_patent_relationships.h5',mode='w')\n",
    "    store.put('/agent_patent_lists', agent_patent_lists)\n",
    "    store.put('/agent_patent_year_lists', agent_patent_year_lists)\n",
    "    store.put('/patent_agent_lists', patent_agent_lists)\n",
    "    store.put('/patent_classes_%s'%class_system, patent_classes)\n",
    "    store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Previous co-agent patenting counts in individual classes \n",
    "def calculate_co_agent_class_counts(agent_patent_lists,\n",
    "                                    agent_patent_year_lists,\n",
    "                                    patent_agent_lists,\n",
    "                                    patent_classes,\n",
    "                                    agents_lookup=agents_lookup,\n",
    "                                    new_class_data=new_class_data,\n",
    "                                    new_class_entries=new_class_entries,\n",
    "                                    ):\n",
    "\n",
    "    n_rows_to_be_created = new_class_data.shape[0]\n",
    "    input_data = new_class_entries.sort(['Agent_ID', \n",
    "                                         'Agent_Class_Number'])[['Agent_ID', \n",
    "                                                                 'Agent_Class_Number', \n",
    "                                                                 'Class_ID', \n",
    "                                                                 'Application_Year']].values \n",
    "    output_data = zeros((n_rows_to_be_created, 5))\n",
    "    output_line = 0\n",
    "    for input_line in arange(len(input_data)):\n",
    "        output_line = multiindex_maker(input_line, input_data, \n",
    "                                       output_line, output_data,\n",
    "                                       function_to_apply=calculate_co_agent_class_counts_helper,\n",
    "                                       function_data=(agents_lookup, \n",
    "                                                      agent_patent_lists, \n",
    "                                                      agent_patent_year_lists, \n",
    "                                                      patent_agent_lists, \n",
    "                                                      patent_classes)\n",
    "                        )\n",
    "\n",
    "    new_class_data.sort(['Agent_ID', 'Agent_Class_Number'], inplace=True)\n",
    "    new_class_data['CoAgent_Previous_Patent_Count_in_Class'] = output_data[:,3]\n",
    "    new_class_data['CoAgent_Count_in_Class'] = output_data[:,4]\n",
    "\n",
    "    return\n",
    "\n",
    "def calculate_co_agent_class_counts_helper(input_line, input_data, \n",
    "                                    output_line, output_data,\n",
    "                                    classes_previously_entered,\n",
    "                                    classes_available,\n",
    "                                    n_reps,\n",
    "                                    other_data):\n",
    "    \n",
    "    year = input_data[input_line,3]\n",
    "    agent = input_data[input_line,0]\n",
    "    \n",
    "    agents_lookup, agent_patent_lists, agent_patent_year_lists, patent_agent_lists, patent_classes = other_data    \n",
    "    \n",
    "    agent = agents_lookup.ix[agent,'Disambiguation_ID']\n",
    " \n",
    "    patents_of_agent = agent_patent_lists.ix[agent]\n",
    "    years_of_patents_of_agent = array(agent_patent_year_lists.ix[agent])\n",
    "    earlier_patent_indices = years_of_patents_of_agent<year\n",
    "    earlier_patents_of_agent = array(patents_of_agent)[earlier_patent_indices]\n",
    "    years_of_earlier_patents_of_agent = years_of_patents_of_agent[earlier_patent_indices]\n",
    "\n",
    "    #Find the sorting to get the most recent patents first\n",
    "    most_recent_sort = argsort(years_of_earlier_patents_of_agent)[::-1] \n",
    "\n",
    "    #Get the agents associated with each patent, sorted with most recent patent first. Note each patent returns a \n",
    "    #list of co-agents, and a co-agent may appear again later in the sequence in an earlier patent.\n",
    "    co_agents_of_earlier_patents = patent_agent_lists.ix[earlier_patents_of_agent[most_recent_sort]]\n",
    "\n",
    "\n",
    "    previous_co_agents = [agent]\n",
    "    previous_patents_of_co_agents = []\n",
    "    previous_classes_of_co_agents = []\n",
    "\n",
    "    for co_agents_with_this_patent, this_patent_year in zip(co_agents_of_earlier_patents, \n",
    "                                         years_of_earlier_patents_of_agent[most_recent_sort]):\n",
    "        for this_co_agent in co_agents_with_this_patent:\n",
    "            if this_co_agent not in previous_co_agents:\n",
    "                previous_co_agents.append(this_co_agent)\n",
    "                #Because we sorted the years from high to low, we record the most recent instance of an agent first. Therefore,\n",
    "                #if we already have the agent in the list, that is the best value, and we don't need to record anything more.\n",
    "\n",
    "                patents_of_co_agent = agent_patent_lists.ix[this_co_agent]\n",
    "                years_of_patents_of_co_agent = agent_patent_year_lists.ix[this_co_agent]\n",
    "                earlier_patents_of_co_agent = [patents_of_co_agent[i] for i in range(len(patents_of_co_agent))\n",
    "                                               if years_of_patents_of_co_agent[i]<this_patent_year]\n",
    "                previous_patents_of_co_agents += earlier_patents_of_co_agent\n",
    "                previous_classes_of_co_agents += list(patent_classes.ix[earlier_patents_of_co_agent, \n",
    "                                                                        'Class_ID'].dropna().unique())\n",
    "\n",
    "    class_co_agent_patent_counts = patent_classes.ix[unique(previous_patents_of_co_agents), \n",
    "                                                     'Class_ID'].dropna().value_counts()\n",
    "    temp = zeros(n_classes)\n",
    "    temp[class_co_agent_patent_counts.index.values.astype('int64')] = class_co_agent_patent_counts.values\n",
    "    class_co_agent_patent_counts = temp[classes_available]\n",
    "\n",
    "    class_co_agent_counts = pd.value_counts(previous_classes_of_co_agents)\n",
    "    temp = zeros(n_classes)\n",
    "    temp[class_co_agent_counts.index.values.astype('int64')] = class_co_agent_counts.values\n",
    "    class_co_agent_counts = temp[classes_available]\n",
    "\n",
    "\n",
    "    output_data[output_line:output_line+n_reps,3] = class_co_agent_patent_counts\n",
    "    output_data[output_line:output_line+n_reps,4] = class_co_agent_counts\n",
    "    \n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### This currently measures both the number of inventions in each class that are near to the agent. \n",
    "### (I.e. the number of patents in each class that the agent's co-agents worked on.)\n",
    "### AND the number of co-agents that had previously been active in each class. \n",
    "### This could be smaller than the number of patents or larger (if you've many collaborators who all worked on one patent\n",
    "### together vs. if you have one collaborator who had many patents before)\n",
    "### Calculating this second quantity roughly doubles the calculation time. Could possibly be optimized.\n",
    "calculate_co_agent_class_counts(agent_patent_lists,\n",
    "                                agent_patent_year_lists,\n",
    "                                patent_agent_lists,\n",
    "                                patent_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rankify('CoAgent_Previous_Patent_Count_in_Class')\n",
    "# rankify('CoAgent_Count_in_Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_class_data['Entered'] = new_class_data['Entered'].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_class_data.drop(['Application_Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in new_class_data.columns:\n",
    "    if new_class_data[c].dtype == 'float64':\n",
    "        new_class_data[c] = new_class_data[c].astype('float32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'agent_entry_data_%s_'%class_system\n",
    "\n",
    "if type(agent_sample)==int:\n",
    "    filename+=('sample_%i_agents'%agent_sample)\n",
    "elif agent_sample is not None:\n",
    "    filename+=('sample_%i_%i_agents'%agent_sample)\n",
    "else:\n",
    "    filename+='_other'\n",
    "    \n",
    "if not use_regressed_z_scores:\n",
    "    filename+='_unregressed_z_scores'\n",
    "    \n",
    "filename+='.h5'\n",
    "\n",
    "store = pd.HDFStore(data_directory+'Agent_Entries/samples/'+filename, complib='blosc',complevel=9, append=False)\n",
    "store['all_available_classes'] = new_class_data\n",
    "store['entries'] = new_class_data[new_class_data.Entered==1]\n",
    "store['class_lookup'] = classes_lookup\n",
    "store.close()"
   ]
  }
 ],
 "metadata": {
  "css": [
   ""
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
