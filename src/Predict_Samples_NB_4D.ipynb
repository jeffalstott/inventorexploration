{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_directory = '../data/'\n",
    "# class_system = 'IPC4'\n",
    "# sample_start = 0\n",
    "# sample_length = 1000\n",
    "# relatedness_types = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_years = arange(1980, 2010, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popularity_types = ['Class_Patent_Count_1_years_Previous_Year_Percentile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7329492568969727\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "data = {}\n",
    "entries = {}\n",
    "pdfs = {}\n",
    "\n",
    "\n",
    "dummy_relatedness = relatedness_types[0]\n",
    "dummy_popularity = popularity_types[0]\n",
    "\n",
    "f = data_directory+'Predictive_Models/PDF/%s/%s/'%(dummy_relatedness,dummy_popularity)\n",
    "for variable in ['Agent_Previous_Citations_to_Class', 'CoAgent_Count_in_Class']:\n",
    "    data[variable] = {}\n",
    "    entries[variable] = {}\n",
    "    pdfs[variable] = {}\n",
    "    for year in model_years:\n",
    "        data[variable][year] = pd.read_hdf(f+'cumulative.h5', 'data/year_%i'%year).astype('float32')\n",
    "        entries[variable][year] = pd.read_hdf(f+'cumulative.h5', 'entries/year_%i'%year).astype('float32')\n",
    "        data[variable][year] = data[variable][year].groupby(level=variable).sum()\n",
    "        entries[variable][year] = entries[variable][year].groupby(level=variable).sum()\n",
    "        pdfs[variable][year] = (entries[variable][year]/data[variable][year]).fillna(0)\n",
    "\n",
    "\n",
    "for variable in relatedness_types:\n",
    "    f = data_directory+'Predictive_Models/PDF/%s/%s/'%(variable,dummy_popularity)\n",
    "    data[variable] = {}\n",
    "    entries[variable] = {}\n",
    "    pdfs[variable] = {}\n",
    "    for year in model_years:\n",
    "        data[variable][year] = pd.read_hdf(f+'cumulative.h5', 'data/year_%i'%year).astype('float32')\n",
    "        entries[variable][year] = pd.read_hdf(f+'cumulative.h5', 'entries/year_%i'%year).astype('float32')\n",
    "        data[variable][year] = data[variable][year].groupby(level=variable).sum()\n",
    "        entries[variable][year] = entries[variable][year].groupby(level=variable).sum()\n",
    "        pdfs[variable][year] = (entries[variable][year]/data[variable][year]).fillna(0)\n",
    "        \n",
    "for variable in popularity_types:\n",
    "    f = data_directory+'Predictive_Models/PDF/%s/%s/'%(dummy_relatedness,variable)\n",
    "    data[variable] = {}\n",
    "    entries[variable] = {}\n",
    "    pdfs[variable] = {}    \n",
    "    for year in model_years:\n",
    "        data[variable][year] = pd.read_hdf(f+'cumulative.h5', 'data/year_%i'%year).astype('float32')\n",
    "        entries[variable][year] = pd.read_hdf(f+'cumulative.h5', 'entries/year_%i'%year).astype('float32')\n",
    "        data[variable][year] = data[variable][year].groupby(level=variable).sum()\n",
    "        entries[variable][year] = entries[variable][year].groupby(level=variable).sum()\n",
    "        pdfs[variable][year] = (entries[variable][year]/data[variable][year]).fillna(0)\n",
    "        \n",
    "\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def rankify(data, data_column,\n",
    "    group_size_indicator, max_group_size=629):\n",
    "\n",
    "    n_data_points= data.shape[0]\n",
    "    output = zeros(n_data_points).astype('float32')\n",
    "    group_start_index = 0\n",
    "\n",
    "    while group_start_index<n_data_points:\n",
    "        group_size = max_group_size-int(data[group_size_indicator].values[group_start_index])+1\n",
    "        group_stop_index = group_start_index + group_size\n",
    "        output[group_start_index:group_stop_index] = rankdata(data[data_column].values[group_start_index:group_stop_index])/group_size\n",
    "        group_start_index = group_stop_index\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def loglossify(data, y_true,y_predicted,\n",
    "    group_size_indicator, max_group_size=629):\n",
    "\n",
    "    n_data_points= data.shape[0]\n",
    "    output = zeros(n_data_points).astype('float32')\n",
    "    group_start_index = 0\n",
    "\n",
    "    while group_start_index<n_data_points:\n",
    "        group_size = max_group_size-int(data[group_size_indicator].values[group_start_index])+1\n",
    "        group_stop_index = group_start_index + group_size\n",
    "        output[group_start_index:group_stop_index] = log_loss(data[y_true].values[group_start_index:group_stop_index].astype('float'),\n",
    "                                                             data[y_predicted].values[group_start_index:group_stop_index].astype('float'))\n",
    "        group_start_index = group_stop_index\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.926220178604126\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "entry_data = pd.HDFStore((data_directory+\n",
    "                                 'Agent_Entries/samples/agent_entry_data_%s_sample_%i_%i_agents.h5'%(class_system,\n",
    "                                                                                                         sample_start,\n",
    "                                                                                                         sample_start+sample_length)))               \n",
    "all_data = entry_data['all_available_classes']\n",
    "\n",
    "all_data['Agent_Previous_Citations_to_Class'] = (all_data['Agent_Previous_Citations_to_Class']>0).astype('int')\n",
    "all_data['CoAgent_Count_in_Class'] = (all_data['CoAgent_Count_in_Class']>0).astype('int')\n",
    "\n",
    "\n",
    "for relatedness in relatedness_types:\n",
    "    if 'z_score' in relatedness:\n",
    "        if 'rescaled' in relatedness:\n",
    "            all_data.ix[all_data[relatedness]<0, relatedness] = 0\n",
    "            f = lambda x: ((x-1)/(x+1))/2+.5\n",
    "            all_data[relatedness] = f(all_data[relatedness].values)#*100\n",
    "            n_bins = 25.0\n",
    "            all_data.ix[all_data[relatedness]==0, relatedness] = -1\n",
    "        else:\n",
    "            n_bins=500.0\n",
    "            \n",
    "    elif 'percent_positive' in relatedness:\n",
    "        n_bins=25.0\n",
    "#         all_data[relatedness] *= 100\n",
    "        #Flag the values that are 0 as -1 so they are below the range of the bins when digitizing.\n",
    "        #They will thus be labeled as \"0\" when digitizing, so we know they were literally 0, and not just close to 0.\n",
    "        all_data.ix[all_data[relatedness]==0, relatedness] = -1\n",
    "\n",
    "    all_data[relatedness] = digitize(all_data[relatedness], arange(0,1, 1/n_bins))/n_bins\n",
    "    \n",
    "for popularity in popularity_types:\n",
    "    n_bins=500.0   \n",
    "    all_data[popularity]/=100\n",
    "    all_data[popularity] = digitize(all_data[popularity], arange(0,1, 1/n_bins))/n_bins\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for relatedness in relatedness_types:\n",
    "    t = time()\n",
    "    print(relatedness)\n",
    "    \n",
    "    for popularity in popularity_types:\n",
    "\n",
    "        for model_year in model_years:\n",
    "            column_label = '%i_NB_4D_with_%s_and_%s'%(model_year, relatedness, popularity)\n",
    "            \n",
    "            p = 1\n",
    "            for variable in [relatedness, popularity, \n",
    "                             'Agent_Previous_Citations_to_Class',\n",
    "                            'CoAgent_Count_in_Class']:\n",
    "                p *= pdfs[variable][model_year][all_data[variable].values].fillna((entries[variable][model_year].sum()/\n",
    "                                                                                   data[variable][model_year].sum())).values\n",
    "            p = p.astype('float32')\n",
    "            \n",
    "            all_data['Prediction_from_'+column_label] = p#model.ix[predictors.to_records(index=False)].fillna(0).values\n",
    "            all_data['Prediction_from_'+column_label] = all_data.groupby(['Agent_ID',\n",
    "                                                                          'Agent_Class_Number'])['Prediction_from_'+column_label].transform(lambda x: x/sum(x))\n",
    "#             all_data['Prediction_from_'+column_label] = all_data['Prediction_from_'+column_label].fillna(0)\n",
    "            \n",
    "            all_data['Prediction_Rank_from_'+column_label] = rankify(all_data, 'Prediction_from_'+column_label, 'Agent_Class_Number')\n",
    "\n",
    "            all_data['Prediction_log_loss_from_'+column_label+''] = loglossify(all_data, 'Entered','Prediction_from_'+column_label,\n",
    "                                                                             'Agent_Class_Number')\n",
    "            del(all_data['Prediction_from_'+column_label])\n",
    "\n",
    "    gc.collect()\n",
    "    print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.2765109539032\n"
     ]
    }
   ],
   "source": [
    "for popularity in popularity_types:\n",
    "    t = time()\n",
    "    for model_year in model_years:\n",
    "        column_label = '%i_NB_3D_with_%s'%(model_year, popularity)\n",
    "\n",
    "        p = 1\n",
    "        for variable in [popularity, \n",
    "                         'Agent_Previous_Citations_to_Class',\n",
    "                        'CoAgent_Count_in_Class']:\n",
    "            p *= pdfs[variable][model_year][all_data[variable].values].fillna((entries[variable][model_year].sum()/\n",
    "                                                                                   data[variable][model_year].sum())).values\n",
    "        p = p.astype('float32')\n",
    "\n",
    "\n",
    "        all_data['Prediction_from_'+column_label] = p#model.ix[predictors.to_records(index=False)].fillna(0).values\n",
    "        all_data['Prediction_from_'+column_label] = all_data.groupby(['Agent_ID',\n",
    "                                                                      'Agent_Class_Number'])['Prediction_from_'+column_label].transform(lambda x: x/sum(x))\n",
    "#             all_data['Prediction_from_'+column_label] = all_data['Prediction_from_'+column_label].fillna(0)\n",
    "\n",
    "        all_data['Prediction_Rank_from_'+column_label] = rankify(all_data, 'Prediction_from_'+column_label, 'Agent_Class_Number')\n",
    "\n",
    "        all_data['Prediction_log_loss_from_'+column_label+''] = loglossify(all_data, 'Entered','Prediction_from_'+column_label,\n",
    "                                                                         'Agent_Class_Number')\n",
    "        del(all_data['Prediction_from_'+column_label])\n",
    "\n",
    "gc.collect()\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entry_data['entries_with_predictions_NB_4D'] = all_data[all_data['Entered']>0]\n",
    "entry_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# movement_data['entries_with_predictions_NB_3D_and_4D'] = all_data[all_data['Entered']>0]\n",
    "# movement_data.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
