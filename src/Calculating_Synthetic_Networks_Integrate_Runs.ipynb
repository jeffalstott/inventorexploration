{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class_system = 'IPC4'\n",
    "# n_controls = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_years = 1#None\n",
    "# if n_years is None or n_years=='all' or n_years=='cumulative':\n",
    "#     n_years_label = ''\n",
    "# else:\n",
    "#     n_years_label = '%i_years_'%n_years\n",
    "\n",
    "# print(n_years_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# entity_types = ['PID']#['Firm', 'Inventor', 'PID']#,  'Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output_citations = 'class_relatedness_networks_citations'\n",
    "# output_cooccurrence = 'class_relatedness_networks_cooccurrence'\n",
    "# combine_outputs = True\n",
    "\n",
    "cooccurrence_base_file_name = 'synthetic_control_cooccurrence_'+n_years_label+'%s_preserve_years_%s'\n",
    "citations_base_file_name = 'synthetic_control_citations_'+n_years_label+'%s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_directory = '../data/'\n",
    "\n",
    "citations_controls_directory = data_directory+'Class_Relatedness_Networks/citations/controls/%s/'%class_system\n",
    "coocurrence_controls_directory = data_directory+'Class_Relatedness_Networks/cooccurrence/controls/%s/'%class_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def running_stats(df_name,\n",
    "                  file_name,\n",
    "                  controls_directory=citations_controls_directory,\n",
    "                  n_controls=n_controls,\n",
    "                 ):\n",
    "    M = None\n",
    "    all_max = None\n",
    "    all_min = None\n",
    "    t = time()\n",
    "    for randomization_id in range(n_controls):\n",
    "\n",
    "        if not randomization_id%10:\n",
    "            print(randomization_id)\n",
    "            print(\"%.0f seconds\"%(time()-t))\n",
    "            t = time()\n",
    "        \n",
    "        f = '%s_%i.h5'%(file_name, randomization_id)\n",
    "        try:\n",
    "            x = pd.read_hdf(controls_directory+f, df_name)\n",
    "        except:\n",
    "            print(\"Data not loading for %s. Continuing.\"%f)\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if M is None:\n",
    "            M = x\n",
    "            S = 0\n",
    "            all_max = M\n",
    "            all_min = M\n",
    "            continue\n",
    "        k = randomization_id+1\n",
    "        M_previous = M\n",
    "        M = M_previous.add( x.subtract(M_previous)/k )\n",
    "        S = ( x.subtract(M_previous).multiply( x.subtract(M) ) ).add(S)\n",
    "        all_max = maximum(all_max, x)\n",
    "        all_min = minimum(all_min, x)\n",
    "        gc.collect()  \n",
    "    standard_deviation = sqrt(S/(k-1))\n",
    "\n",
    "    return M, standard_deviation, all_max, all_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if output_citations:\n",
    "    M, standard_deviation, all_max, all_min = running_stats('synthetic_citations_%s'%class_system,\n",
    "                                      citations_base_file_name%class_system,\n",
    "                                      citations_controls_directory\n",
    "                                     )\n",
    "\n",
    "    store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/citations/%s.h5'%(output_citations),\n",
    "                        mode='a', table=True)\n",
    "    store.put('/randomized_mean_%s%s'%(n_years_label, class_system), M, 'table', append=False)\n",
    "    store.put('/randomized_std_%s%s'%(n_years_label, class_system), standard_deviation, 'table', append=False)\n",
    "\n",
    "    store.put('/randomized_max_%s%s'%(n_years_label, class_system), all_max, 'table', append=False)\n",
    "    store.put('/randomized_min_%s%s'%(n_years_label, class_system), all_min, 'table', append=False)\n",
    "\n",
    "    z_scores = store['empirical_citations_%s%s'%(n_years_label, class_system)].ix[M.labels].subtract(M).divide(standard_deviation)\n",
    "\n",
    "    z_scores.values[where(z_scores==inf)]=nan \n",
    "    #All the cases where the z-scores are inf is where the 1,000 randomized controls said there should be 0 deviation, BUT\n",
    "    #the empirical case was different anyway. In each of these cases, the empirical case was JUST slightly off. Sometimes\n",
    "    #a floating point error, and sometimes off by 1 (the minimal amount for citation counts). We shall treat this as not actually\n",
    "    #deviating, and so it becomes 0/0, which is equal to nan.\n",
    "\n",
    "    store.put('/empirical_citations_z_scores_%s%s'%(n_years_label, class_system), z_scores, 'table', append=False)\n",
    "\n",
    "    store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 seconds\n",
      "10\n",
      "20 seconds\n",
      "20\n",
      "28 seconds\n",
      "30\n",
      "28 seconds\n",
      "40\n",
      "28 seconds\n",
      "50\n",
      "28 seconds\n",
      "60\n",
      "28 seconds\n",
      "70\n",
      "28 seconds\n",
      "80\n",
      "28 seconds\n",
      "90\n",
      "28 seconds\n",
      "100\n",
      "42 seconds\n",
      "110\n",
      "35 seconds\n",
      "120\n",
      "31 seconds\n",
      "130\n",
      "28 seconds\n",
      "140\n",
      "31 seconds\n",
      "150\n",
      "32 seconds\n",
      "160\n",
      "32 seconds\n",
      "170\n",
      "32 seconds\n",
      "180\n",
      "31 seconds\n",
      "190\n",
      "32 seconds\n",
      "200\n",
      "32 seconds\n",
      "210\n",
      "32 seconds\n",
      "220\n",
      "32 seconds\n",
      "230\n",
      "32 seconds\n",
      "240\n",
      "32 seconds\n",
      "250\n",
      "32 seconds\n",
      "260\n",
      "32 seconds\n",
      "270\n",
      "32 seconds\n",
      "280\n",
      "31 seconds\n",
      "290\n",
      "32 seconds\n",
      "300\n",
      "33 seconds\n",
      "310\n",
      "34 seconds\n",
      "320\n",
      "32 seconds\n",
      "330\n",
      "34 seconds\n",
      "340\n",
      "31 seconds\n",
      "350\n",
      "31 seconds\n",
      "360\n",
      "31 seconds\n",
      "370\n",
      "31 seconds\n",
      "380\n",
      "31 seconds\n",
      "390\n",
      "31 seconds\n",
      "400\n",
      "31 seconds\n",
      "410\n",
      "31 seconds\n",
      "420\n",
      "31 seconds\n",
      "430\n",
      "31 seconds\n",
      "440\n",
      "31 seconds\n",
      "450\n",
      "31 seconds\n",
      "460\n",
      "31 seconds\n",
      "470\n",
      "31 seconds\n",
      "480\n",
      "31 seconds\n",
      "490\n",
      "31 seconds\n",
      "500\n",
      "31 seconds\n",
      "510\n",
      "31 seconds\n",
      "520\n",
      "31 seconds\n",
      "530\n",
      "30 seconds\n",
      "540\n",
      "30 seconds\n",
      "550\n",
      "30 seconds\n",
      "560\n",
      "32 seconds\n",
      "570\n",
      "31 seconds\n",
      "580\n",
      "31 seconds\n",
      "590\n",
      "28 seconds\n",
      "600\n",
      "28 seconds\n",
      "610\n",
      "28 seconds\n",
      "620\n",
      "28 seconds\n",
      "630\n",
      "28 seconds\n",
      "640\n",
      "28 seconds\n",
      "650\n",
      "28 seconds\n",
      "660\n",
      "28 seconds\n",
      "670\n",
      "28 seconds\n",
      "680\n",
      "28 seconds\n",
      "690\n",
      "28 seconds\n",
      "700\n",
      "28 seconds\n",
      "710\n",
      "28 seconds\n",
      "720\n",
      "29 seconds\n",
      "730\n",
      "31 seconds\n",
      "740\n",
      "32 seconds\n",
      "750\n",
      "32 seconds\n",
      "760\n",
      "32 seconds\n",
      "770\n",
      "32 seconds\n",
      "780\n",
      "32 seconds\n",
      "790\n",
      "32 seconds\n",
      "800\n",
      "32 seconds\n",
      "810\n",
      "32 seconds\n",
      "820\n",
      "32 seconds\n",
      "830\n",
      "32 seconds\n",
      "840\n",
      "32 seconds\n",
      "850\n",
      "31 seconds\n",
      "860\n",
      "32 seconds\n",
      "870\n",
      "32 seconds\n",
      "880\n",
      "32 seconds\n",
      "890\n",
      "32 seconds\n",
      "900\n",
      "31 seconds\n",
      "910\n",
      "31 seconds\n",
      "920\n",
      "32 seconds\n",
      "930\n",
      "31 seconds\n",
      "940\n",
      "31 seconds\n",
      "950\n",
      "32 seconds\n",
      "960\n",
      "32 seconds\n",
      "970\n",
      "32 seconds\n",
      "980\n",
      "33 seconds\n",
      "990\n",
      "32 seconds\n"
     ]
    }
   ],
   "source": [
    "if output_cooccurrence:\n",
    "    M = None\n",
    "    for entity in entity_types:\n",
    "        print(entity)\n",
    "        (M_entity, \n",
    "         standard_deviation_entity, \n",
    "         all_max_entity, \n",
    "         all_min_entity) = running_stats('synthetic_cooccurrence_%s_%s'%(entity, class_system),\n",
    "                                          cooccurrence_base_file_name%(entity, class_system),\n",
    "                                          coocurrence_controls_directory\n",
    "                                         )\n",
    "        if M is None:\n",
    "            M = pd.Panel4D({'Class_CoOccurrence_Count_%s'%entity: M_entity})\n",
    "            standard_deviation = pd.Panel4D({'Class_CoOccurrence_Count_%s'%entity: standard_deviation_entity})\n",
    "            all_max = pd.Panel4D({'Class_CoOccurrence_Count_%s'%entity: all_max_entity}) \n",
    "            all_min = pd.Panel4D({'Class_CoOccurrence_Count_%s'%entity: all_min_entity})\n",
    "        else:\n",
    "            M['Class_CoOccurrence_Count_%s'%entity] = M_entity\n",
    "            standard_deviation['Class_CoOccurrence_Count_%s'%entity] = standard_deviation_entity\n",
    "            all_max['Class_CoOccurrence_Count_%s'%entity] = all_max_entity\n",
    "            all_min['Class_CoOccurrence_Count_%s'%entity] = all_min_entity\n",
    "\n",
    "    store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/cooccurrence/%s.h5'%(output_cooccurrence),\n",
    "                        mode='a', table=True)\n",
    "    store.put('/randomized_mean_%s%s'%(n_years_label, class_system), M, 'table', append=False)\n",
    "    store.put('/randomized_std_%s%s'%(n_years_label, class_system), standard_deviation, 'table', append=False)\n",
    "\n",
    "    store.put('/randomized_max_%s%s'%(n_years_label, class_system), all_max, 'table', append=False)\n",
    "    store.put('/randomized_min_%s%s'%(n_years_label, class_system), all_min, 'table', append=False)\n",
    "\n",
    "    try:\n",
    "        z_scores = store['empirical_cooccurrence_%s%s'%(n_years_label, class_system)].ix[M.labels].subtract(M).divide(standard_deviation)\n",
    "\n",
    "        z_scores.values[where(z_scores==inf)]=nan \n",
    "        #All the cases where the z-scores are inf is where the 1,000 randomized controls said there should be 0 deviation, BUT\n",
    "        #the empirical case was different anyway. In each of these cases, the empirical case was JUST slightly off. Sometimes\n",
    "        #a floating point error, and sometimes off by 1 (the minimal amount for citation counts). We shall treat this as not actually\n",
    "        #deviating, and so it becomes 0/0, which is equal to nan.\n",
    "\n",
    "        store.put('/empirical_cooccurrence_z_scores_%s%s'%(n_years_label, class_system), z_scores, 'table', append=False)\n",
    "    except KeyError:\n",
    "        print(\"No empirical data saved to calculate z-scores with\")\n",
    "        pass\n",
    "        \n",
    "    store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining Outputs\n"
     ]
    }
   ],
   "source": [
    "if combine_outputs:\n",
    "    print(\"Combining Outputs\")\n",
    "    citation_store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/citations/class_relatedness_networks_citations.h5')\n",
    "    cooccurrence_store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/cooccurrence/class_relatedness_networks_cooccurrence.h5')\n",
    "    \n",
    "    M = citation_store['/randomized_mean_%s%s'%(n_years_label, class_system)]\n",
    "    standard_deviation = citation_store['/randomized_std_%s%s'%(n_years_label, class_system)]\n",
    "    all_max = citation_store['/randomized_max_%s%s'%(n_years_label, class_system)]\n",
    "    all_min = citation_store['/randomized_min_%s%s'%(n_years_label, class_system)]\n",
    "#     z_scores = citation_store['/empirical_citations_z_scores_%s%s'%(n_years_label, class_system)]\n",
    "    z_scores = citation_store['empirical_citations_%s%s'%(n_years_label, class_system)].ix[M.labels].subtract(M).divide(standard_deviation)\n",
    "    z_scores.values[where(z_scores==inf)]=nan \n",
    "  \n",
    "    M_c = cooccurrence_store['/randomized_mean_%s%s'%(n_years_label, class_system)]\n",
    "    standard_deviation_c = cooccurrence_store['/randomized_std_%s%s'%(n_years_label, class_system)]\n",
    "    all_max_c = cooccurrence_store['/randomized_max_%s%s'%(n_years_label, class_system)]\n",
    "    all_min_c = cooccurrence_store['/randomized_min_%s%s'%(n_years_label, class_system)]\n",
    "#     z_scores_c = cooccurrence_store['/empirical_cooccurrence_z_scores_%s%s'%(n_years_label, class_system)]\n",
    "    z_scores_c = cooccurrence_store['empirical_cooccurrence_%s%s'%(n_years_label, class_system)].ix[M_c.labels].subtract(M_c).divide(standard_deviation_c)\n",
    "    z_scores_c.values[where(z_scores_c==inf)]=nan \n",
    "    \n",
    "    for label in M_c.labels:\n",
    "        M[label] = M_c[label]\n",
    "        standard_deviation[label] = standard_deviation_c[label]\n",
    "        all_max[label] = all_max_c[label]\n",
    "        all_min[label] = all_min_c[label]\n",
    "        z_scores[label] = z_scores_c[label]\n",
    "        \n",
    "    \n",
    "    combine_store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/class_relatedness_networks.h5', \n",
    "                                mode='a', table=True)\n",
    "   \n",
    "    combine_store.put('/randomized_mean_%s%s'%(n_years_label, class_system), M, 'table', append=False)\n",
    "    combine_store.put('/randomized_std_%s%s'%(n_years_label, class_system), standard_deviation, 'table', append=False)\n",
    "\n",
    "    combine_store.put('/randomized_max_%s%s'%(n_years_label, class_system), all_max, 'table', append=False)\n",
    "    combine_store.put('/randomized_min_%s%s'%(n_years_label, class_system), all_min, 'table', append=False)\n",
    "\n",
    "    combine_store.put('/empirical_z_scores_%s%s'%(n_years_label, class_system), z_scores, 'table', append=False)\n",
    "\n",
    "    combine_store.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "css": [
   ""
  ],
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
